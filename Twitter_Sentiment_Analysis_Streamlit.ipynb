{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Twitter Sentiment Analysis Streamlit.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNWm6PWExuH+ZaANM5ynIuk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amitpanthi/Twitter-Sentiment-Analysis/blob/main/Twitter_Sentiment_Analysis_Streamlit.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FxSgRU-DVFYW"
      },
      "source": [
        "!pip install streamlit"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WG27_IxFVlfh",
        "outputId": "f9735229-d3ea-400c-c949-dfdd8a5b4f90"
      },
      "source": [
        "%%writefile model.py\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class SentimentPredictor(nn.Module):\n",
        "    def __init__(self, input_size, embedding_size, hidden_size, num_lstm_layers, pad_token):\n",
        "        super(SentimentPredictor, self).__init__()\n",
        "        \n",
        "        self.embedding = nn.Embedding(num_embeddings=input_size, embedding_dim=embedding_size, padding_idx=pad_token)\n",
        "        self.lstm = nn.LSTM(embedding_size, hidden_size, num_lstm_layers, dropout=0.5)\n",
        "        self.dense1 = nn.Linear(hidden_size, 64)\n",
        "        self.dense2 = nn.Linear(64, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        x, _ = self.lstm(x)\n",
        "        x = torch.relu(self.dense1(x[-1, :, :])) # get hidden size first\n",
        "        x = self.dense2(x)\n",
        "\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing model.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "53x355JLV7-6",
        "outputId": "e49dcd21-6f72-456e-9ec2-8ffb0089c3bc"
      },
      "source": [
        "%%writefile predict.py\n",
        "import torch\n",
        "from nltk import RegexpTokenizer\n",
        "from model import SentimentPredictor\n",
        "\n",
        "class Predictor:\n",
        "    def __init__(self, pth_file):\n",
        "        checkpoint = torch.load(pth_file)\n",
        "        hparams = checkpoint[\"hparams\"]\n",
        "        model_params = checkpoint[\"model_params\"]\n",
        "        self.stoi = checkpoint[\"stoi\"]\n",
        "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "        self.model = SentimentPredictor(hparams[\"input_size\"], hparams[\"embedding_size\"], \n",
        "                                        hparams[\"hidden_size\"], hparams[\"num_layers\"], hparams[\"pad_token\"]).to(self.device)\n",
        "        self.model.load_state_dict(model_params)\n",
        "\n",
        "\n",
        "    def get_sentiment(self, tweet):\n",
        "        self.model.eval()\n",
        "        tweet = tweet.lower()\n",
        "        tokenizer = RegexpTokenizer(r\"\\w+\")\n",
        "        tokens = tokenizer.tokenize(tweet)\n",
        "        tweet_tokens = [self.stoi.get(token, 0) for token in tokens]\n",
        "        tweet_tokens = torch.Tensor(tweet_tokens).long().to(self.device)\n",
        "        tweet_tokens = torch.transpose(tweet_tokens.unsqueeze(0), 0, 1)\n",
        "\n",
        "        output = self.model(tweet_tokens)\n",
        "        output = torch.sigmoid(output).item()\n",
        "\n",
        "        if output >= 0.42 and output <= 0.57:\n",
        "            return f\"Score: {output:.2f}. Neutral\"\n",
        "        elif output > 0.57:\n",
        "            return f\"Score: {output:.2f}. Positive\"\n",
        "        else:\n",
        "            return f\"Score: {output:.2f}. Negative\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing predict.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QPrNnmg4ZV7h",
        "outputId": "fdeaf340-2db7-440b-d537-946f1a572e71"
      },
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "from predict import Predictor\n",
        "\n",
        "def main():\n",
        "    sa_pred = Predictor(\"sentiment.pth\")\n",
        "    st.set_page_config(page_title=\"Tweet Sentiment Predictor\", page_icon=\"ðŸ¤¬\")\n",
        "    st.title(\"Tweet Sentiment Predictor\")\n",
        "    user_input = st.text_input(\"Enter your tweet here\")\n",
        "\n",
        "    if st.button(\"Find Sentiment\"):\n",
        "        st.write(f\"{sa_pred.get_sentiment(user_input)}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing app.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tCv4QO9jbxha"
      },
      "source": [
        "!pip install pyngrok"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bEJeKQ15b2QX"
      },
      "source": [
        "!streamlit run app.py &>/dev/null&"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZQlJY90Tb4Sq",
        "outputId": "9f12bd54-a48b-4b74-dc64-d69d0b87ce96"
      },
      "source": [
        "from pyngrok import ngrok\n",
        "!ngrok authtoken <your auth token>"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Authtoken saved to configuration file: /root/.ngrok2/ngrok.yml\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xe7BihBPb68I"
      },
      "source": [
        "# Setup a tunnel to the streamlit port 8501\n",
        "public_url = ngrok.connect(8501)\n",
        "public_url"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MplSnVufb9Fc"
      },
      "source": [
        "ngrok.kill()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JBTRvzBEdEAp"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}